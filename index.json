[
{
	"uri": "http://baoandev.github.io/5-queryandvisualize/5.1-athenaandquicksight/",
	"title": "Athena and Quicksight",
	"tags": [],
	"description": "",
	"content": "Getting Started In this lab, you will complete the following tasks:\nQuery data and create a view with Amazon Athena Athena Workgroups to Control Query Access and Costs Build a dashboard with Amazon QuickSight Query Data with Amazon Athena Get start Athena\nIn the AWS services console, search for Athena.\nClick Get Started Then click set up a query result location in Amazon S3 at the top In the pop-up window in the Query result location field, click the Select icon. Choose the bucket with the string dmslabs3bucket ( e.g: dmslab-student-dmslabs3bucket-xg1hdyq60ibs), then click on Select button. Append athenaquery/ at the end of the S3 location, don\u0026rsquo;t forgot the / at the end. Click on Save.\nIn the Query Editor, select your newly created database e.g., \u0026ldquo;ticketdata”. Click the table named \u0026ldquo;parquet_sporting_event_ticket\u0026rdquo; to inspect the fields. Note: The type for fields id, sporting_event_id and ticketholder_id should be (double). Next, we will query across tables parquet_sporting_event, parquet_sport_team, and parquet_sport location.\nCopy the following SQL syntax into the Query 1 tab and click Run. SELECT e.id AS event_id, e.sport_type_name AS sport, e.start_date_time AS event_date_time, h.name AS home_team, a.name AS away_team, l.name AS location, l.city FROM parquet_sporting_event e, parquet_sport_team h, parquet_sport_team a, parquet_sport_location l WHERE e.home_team_id = h.id AND e.away_team_id = a.id AND e.location_id = l.id; The results appear beneath the query window. As shown above Click Create and then select View from query Name the view sporting_event_info and click Create Your new view is created\nCopy the following SQL syntax into the Query 3 tab. SELECT t.id AS ticket_id, e.event_id, e.sport, e.event_date_time, e.home_team, e.away_team, e.location, e.city, t.seat_level, t.seat_section, t.seat_row, t.seat, t.ticket_price, p.full_name AS ticketholder FROM sporting_event_info e, parquet_sporting_event_ticket t, parquet_person p WHERE t.sporting_event_id = e.event_id AND t.ticketholder_id = p.id Click on Save as button Give this query a name: create_view_sporting_event_ticket_info and some description and then, click on Save query. Back to the query editor, you will see the query name changed. Now, click on ­­Run\nThe results appear beneath the query window. As shown above, click View from query.\nName the view sporting_event_ticket_info and click Create. Copy the following SQL syntax into the Query 5 tab.\nSELECT sport, count(distinct location) as locations, count(distinct event_id) as events, count(*) as tickets, avg(ticket_price) as avg_ticket_price FROM sporting_event_ticket_info GROUP BY 1 ORDER BY 1; Click on Save as and give this query name: analytics_sporting_event_ticket_info and some description and then, click on Save. The name of the New Query 5 will be changed to one assigned in previous step. Click on Run.\nYou query returns two results in approximately five seconds. The query scans 25 MB of data, which prior to converting to parquet, would have been 1.59GB of CSV files.\nThe purpose of saving the queries is to have clear distinction between the results of the queries running on one view. Otherwise, your query results will be saved under “Unsaved” folder within the S3 bucket location provided to Athena to store query results. Please navigate to S3 bucket to observe these changes, as shown below:\nBuild an Amazon QuickSight Dashboard Set up QuickSight In the AWS services console, search for QuickSight. If this is the first time you have used QuickSight, you are prompted to create an account.\nClick Sign up for QuickSight. Select your DMS bucket (e.g., \u0026ldquo;xxx-dmslabs3bucket-xxx\u0026rdquo;), Click Finish. On the top right corner, click New analysis. Click New dataset. On the Create a Dataset page, select Athena as the data source. For Data source name, type ticketdata-qs , then click Validate connection. Click Create data source.\nIn the Database drop-down list, select the database ticketdata.\nChoose the \u0026ldquo;sporting_event_ticket_info\u0026rdquo; table and click Select. To finish data set creation, choose the option Import to SPICE for quicker analytics and click Visualize. If your SPICE has 0 bytes available, choose the second choice Directly query your data. You will now be taken to the QuickSight Visualize interface where you can start building your dashboard. Create QuickSight Charts In this section i will take you through some of the different chart types.\nIn the Fields list, click the ticket_price column to populate the chart.\nClick the expand icon in corner of \u0026ldquo;ticket_price\u0026rdquo; field, and select Show as Currency to show the number in dollar value. You can add visual by clicking Add button from the Visuals pane of the screen.\nIn the Visual types area, choose the Vertical bar chart icon. This layout requires a value for the X-axis. In Fields list, select the event_date_time field and you should see the visualization update. For Value Y-axis, select “ticket_price” from the Field list. You can drag and move other visuals to adjust space in dashboard. In the Fields list, click and drag the seat_level field to the Group/Color box. You can also use the slider below the x axis to fit all of the data. Let’s build on this one step further by changing the chart type:\nIn the Visuals pane, click Vertical bar chart under change visual type and choose the Clustered bar combo chart icon.\nIn the Fields list, click and drag the ticketholder field to the Lines box.\nIn the Lines box, click the dropdown box and choose Aggregate: Count Distinct for Aggregate. You can then see the y-axis update on the right-hand side. Create QuickSight Parameters In the next section we are going to create some parameters with controls for the dashboard, then assign these to a filter for all the visuals.\nFrom the Tool bar, select Parameters icon. Click Add to create a new parameter with a Name.\nFor Name, type EventFrom.\nFor Data type, choose Datetime.\nFor Time granularity, set Hour.\nFor Default value, select the value from calendar as start date available in your graph for event_date_time. For example, 2021-01-01 00:00.\nClick Create, and then close the Parameter Added dialog box. Create another parameter with the following attributes:\nName: EventTo Data type: Datetime For Time granularity, set Hour. For Default value, select the value from calendar as end date available in your graph for event_date_time. For example, 2022-01-01 00:00 Click Create In next window, you can select any option to perform any operation with the parameter. Alternatively, you can click the three dots for the EventFrom parameter and choose Add control. For Display name, specify Event From and click Add. Repeat the process to add a control for EventTo with display name Event To\nYou should now be able to see and expand the Controls section above the chart. Create a QuickSight Filter To complete the process, we will wire up a filter to these controls for all visuals.\nFrom the Tool bar, choose Filter.\nClick the plus icon (+) to add a filter for the field event_date_time. Click this filter to edit the properties.\nSelect All applicable visuals in the Applied To dropdown\nFor Filter type, choose Date \u0026amp; Time range and Condition as Between.\nSelect option Use Parameter, click Yes to apply to all visual.\nFor Start date parameter, choose EventFrom.\nFor End date parameter, choose EventTo. Click Apply.\nAdd Calculated Fields In the next section, you will learn, how to add calculated fields for \u0026ldquo;day of week\u0026rdquo; and \u0026ldquo;hour of day\u0026rdquo; to your dataset and a new scatter plot for these two dependent variables.\nClick the Data menu and select Add Calculated Field. Give it a name event_day_of_week\nFor Formula, type extract(\u0026lsquo;WD\u0026rsquo;,{event_date_time})\nClick Save Add another calculated field with the following attributes:\nCalculated field name: event_hour_of_day Formula: extract(\u0026lsquo;HH\u0026rsquo;,{event_date_time}) Click Visualize icon from the Tool bar and choose Add visual. For field type, select the scatter plot.\nIn the Fields list, click the following attributes to set the graph attributes:\nX-axis: \u0026ldquo;event_hour_of_day\u0026rdquo; Y-axis: \u0026ldquo;event_day_of_week\u0026rdquo; Size: \u0026ldquo;ticket_price\u0026rdquo; You can now click on Publish on the top right corner of screen to publish your visuals. You can also, share it by clicking on the Share menu.\nA dashboard is a read-only snapshot of an analysis that you can share with other Amazon QuickSight users for reporting purposes. In Dashboard other users can still play with visuals and data but that will not modify dataset.\nYou can share an analysis with one or more other users with whom you want to collaborate on creating visuals. Analysis provides other uses to write and modify data set.\n"
},
{
	"uri": "http://baoandev.github.io/4-transformingdatawithglue/4.1-datavalidationandetl/",
	"title": "Data Validation and ETL",
	"tags": [],
	"description": "",
	"content": "Create Glue Crawler for initial full load data Navigate to the AWS Glue Console\nOn the AWS Glue menu, under \u0026lsquo;Data Catalog\u0026rsquo;, select Crawlers. Click Create crawler.\nEnter glue-lab-crawler as the crawler name for initial data load.\nOptionally, enter the description. This should also be descriptive and easily recognized and Click Next. Under Choose data sources and classifiers select Not yet and click Add a data source On the Add a data store page, make the following selections:\nFor Data source, click the drop-down box and select S3. For Location of S3 data, select In this account For S3 path, browse to the target folder stored CSV files, e.g., s3://xxx-dmslabs3bucket-xxx/tickets/ Leave all other parameters as default Click Add an S3 data source Click Next. On the Configure security settings page, make the following selection:\nUnder Existing IAM role select -GlueLabRole- which is pre-created for you. For example “mod-3fccddd123456789-GlueLabRole-ZOQDII7JTBUM” Click Next. On the Set output and scheduling, under Target database, click Add database which pops-up another tab. Enter ticketdata as your database name and click Create database Please get back to the Set output and scheduling page\nSelect Target database as \u0026rsquo;ticketdata\u0026rsquo;. Please use the refresh option right next to the dropdown to make sure that the new database we created is listing there. For Prefix added to tables (optional), leave the field empty. Under Crawler schedule as select the frequency as \u0026lsquo;On demand\u0026rsquo; Click Next Review the information and click Create crawler. The crawler is now ready to run.\nExecute the Crawler by clicking Run crawler button. Crawler will change status from \u0026lsquo;Running\u0026rsquo; to \u0026lsquo;Stopping\u0026rsquo;, wait until crawler comes back to \u0026lsquo;Ready\u0026rsquo; state (the process will take a few minutes), you can see that it has created 15 tables. In the AWS Glue navigation pane, under Data Catalog click Databases → Tables. You can also click the ticketdata database to browse the tables. Data Validation Exercise Within the Tables section of your ticketdata database, click the person table. You may have noticed that some tables (such as person) have column headers such as col0,col1,col2,col3. In absence of headers or when the crawler cannot determine the header type, default column headers are specified.\nThis exercise uses the person table in an example of how to resolve this issue.\nClick Edit Schema option In the Edit Schema section, select the row for col0 (column name) and click Edit. Type id as the column name and click Save. Repeat the above steps to change the remaining column names to match those shown in the following figure: full_name, last_name and first_name. Click Save as new table version. Data ETL Exercise Pre-requisite: To store processed data in parquet format, we need a new folder location for each table, eg. the full path for sport_team table look like this - s3://\u0026lt;s3_bucket_name\u0026gt;/tickets/dms_parquet/sport_team\nGlue will create the new folder automatically, based on your input of the full file path, such as the example above. Please refer to the user guide in terms of how to manually create a folder in S3 bucket.\nIn the left navigation pane, click ETL jobs.\nClick “Visual ETL” Select Amazon S3 from the Sources list to add a Data source - S3 bucket node. Select the Data source - S3 bucket node to show the data source properties.\nIn the panel on the right under “Data source properties - S3”, select Data Catalog table and choose the ticketdata database from the drop down. For Table, select the sport_team table.\nClick on the + button and select Change Schema from the Transforms list to add a Transform - Change Schema node. Select the Transform - Change Schema node to show the properties. In the Transform panel on the right, change the data type of id column to double in the dropdown. Click on the + button and select Amazon S3 from the Targets list to add a Data target - S3 bucket node. Select the Data target - S3 bucket node to show the properties. In the panel on the right, change the Format to Parquet in the dropdown. Under Compression Type, select Uncompressed from the dropdown.\nUnder “S3 Target Location”, select “Browse S3” browse to the “mod-xxx-dmslabs3bucket-xxx” bucket, select “tickets” item and press “Choose”. In the textbox, append dms_parquet/sport_team/ to the S3 url. The path should look similar to s3://mod-xxx-dmslabs3bucket-xxx/tickets/dms_parquet/sport_team/ - don’t forget the / at the end. The job will automatically create the folder. Finally, select the Job details tab at the top. Enter Glue-Lab-SportTeamParquet under Name.\nFor IAM Role, select the role named similar to mod-xxx-GlueLabRole-xxx.\nScroll down the page and under Job bookmark, select Disable in the drop down. You can try out the bookmark functionality later in this lab. Press the Save button in the top right-hand corner to create the job.\nOnce you see the Successfully created job message in the banner, click the Run button to start the job.\nSelect Jobs from the navigation panel on the left-hand side to see a list of your jobs.\nSelect Monitoring from the navigation panel on the left-hand side to view your running jobs, success/failure rates and various other statistics. Scroll down to the Job runs list to verify that the ETL job has completed successfully. This should take about 1 minute to complete.\nWe need to repeat this process for an additional 4 jobs, to transform the sport_location, sporting_event, sporting_event_ticket and person tables.\nDuring this process, we will need to modify different column data types. We can either repeat the process above for each table, or we can clone the first job and update the details. The steps below describe how to clone the job - if creating manually each time, follow the above steps but make sure you use the updated values from the tables below.\nReturn to the Jobs menu, and select the Glue-Lab-SportsTeamParquet job by clicking the checkbox next to the name. Under the Actions dropdown, select Clone job. Update the job as per the following tables, then Save and Run.\nSport_Location:\nCreate a Glue-Lab-SportLocationParquet job with the following attributes:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sport_location Transform - ApplyMapping node Schema transformations None Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sport_location/ Job details tab Job Name Glue-Lab-SportLocationParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Sporting_Event:\nCreate a Glue-Lab-SportingEventParquet job with the following attributes:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sporting_event Transform - ApplyMapping node Schema transformations column “start_date_time” =\u0026gt; TIMESTAMP column “start_date” =\u0026gt; DATE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sporting_event/ Job details tab Job Name Glue-Lab-SportingEventParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Sporting_Event_Ticket:\nCreate a Glue-Lab-SportingEventTicketParquet job with the following attributes:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sporting_event_ticket Transform - ApplyMapping node Schema transformations column “id” =\u0026gt; DOUBLE column “sporting_event_id” =\u0026gt; DOUBLE column “ticketholder_id” =\u0026gt; DOUBLE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sporting_event_ticket/ Job details tab Job Name Glue-Lab-SportingEventTicketParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Person\nCreate a Glue-Lab-PersonParquet job with the following attributes:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table person Transform - ApplyMapping node Schema transformations column “id” =\u0026gt; DOUBLE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/person/ Job details tab Job Name Glue-Lab-PersonParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Create Glue Crawler for Parquet Files In the Glue navigation menu, under Data Catalog select Crawlers. Click Create crawler. For Crawler name, type glue-lab-parquet-crawler and Click Next. In the Add data store screen\nFor Choose a data store, select “S3”. For Location of S3 data, select “In this account”. For S3 path, specify the S3 Path (Parent Parquet folder) that contains the nested parquet files e.g., s3://xxx-dmslabs3bucket-xxx/tickets/dms_parquet/ Leave all other parameters as default Click Add an S3 data source. In next screen Choose data sources and classifiers, click Next. On the Configure security settings page, select Choose an existing IAM role. For IAM role, select the existing role “xxx-GlueLabRole-xxx” and Click Next. On the Set output and scheduling page, Under Target database, choose your existing database which you created earlier e.g. ticketdata For the Prefix added to tables (optional), type parquet_ For Crawler schedule select “On Demand” and Click Next. Review the summary page and click Create crawler. Click Run Crawler. Once your crawler has finished running, you should see that that tables were added, depending on how many parquet ETL conversions you set up in the previous section. Confirm you can see the tables In the left navigation pane, click Tables. Add the filter parquet for Classification to return the newly created tables. "
},
{
	"uri": "http://baoandev.github.io/",
	"title": "Data with AWS",
	"tags": [],
	"description": "",
	"content": "Data Engineering Immersion Day What is a Data Engineering Immersion Day? The Data Engineering Immersion Day has hands-on labs and modules that focus on ingestion, hydration, exploration, and consumption of data in a data lake in AWS.\nBenefits of a Data Engineering Immersion Day. The Data Engineering Immersion day allows hands-on time with AWS analytics services including Amazon Kinesis Services for streaming data ingestion and analytics, AWS Data Migration service for batch data ingestion, AWS Glue for data catalog and run ETL on Data lake, Amazon Athena to query data lake, and Amazon Quicksight for visualization. This Immersion day helps to build a cloud-native and future-proof serverless data lake.\nContent Introduction Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink Ingestion with DMS Transforming data with Glue Query and Visualize Glue DataBrew "
},
{
	"uri": "http://baoandev.github.io/7-gluedatabrew/7.1-prelab/",
	"title": "DataBrew Prelab",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab, we will be using AWS Glue DataBrew to explore a dataset in S3, and to clean and prepare the data.\nTo do this, we will first set up an IAM role to use in DataBrew, and an S3 bucket for the results from the DataBrew jobs.\nCloudFormation Stack Deployment Click the Deploy to AWS link below to create the AWS resources for the DataBrew lab. https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?templateURL=https://s3.us-east-1.amazonaws.com/aws-dataengineering-day.workshop.aws/DataBrew_PreLab_CFN.yaml\u0026stackName=databrew-lab\u0026param_SourceBucket=aws-dataengineering-day.workshop.aws\u0026param_SourceKey=states_daily.csv.gz\nCheck the box \u0026ldquo;I acknowledge that \u0026hellip;\u0026rdquo;, then click on \u0026ldquo;Create Stack\u0026rdquo; to create the stack. This should take about a minute to complete creating the stack. Once your stack is deployed, click the Outputs tab to view more information Note the values for DatasetS3Path, DataBrewLabRole and DataBrewOutputS3Bucket which will be used in the lab.\nCongratulations! You are all done with the CloudFormation deployment.\n"
},
{
	"uri": "http://baoandev.github.io/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "What you\u0026rsquo;ll do in these labs: These labs are designed to be completed in sequence, and the full set of instructions are documented below. Read and follow along to complete the labs. Our lab instructor will give you a high-level overview of the labs and help answer any questions. Don\u0026rsquo;t worry if you get stuck, we provide hints along the way.\nArchitecture Patterns Perform Data Transformations Finally you will apply Machine Learning "
},
{
	"uri": "http://baoandev.github.io/3-ingestion/3.1-skipdms/",
	"title": "Option 3: Skip DMS",
	"tags": [],
	"description": "",
	"content": "Prepare data for Module 4 vs 5 Steps Open AWS CloudShell Copy the data from the staging Amazon S3 bucket to your S3 bucket Verify the data About the lab setup: An RDS Postgres Database is used as the source of ticket sales for sporting events. It stores transaction information about the ticket sales price to selected people and ticket ownership transfers with additional tables for event details. AWS Database Migration Service (DMS) is used for a full data load from the Amazon RDS source to Amazon S3 bucket.\nBefore the Glue lab starts, you might choose to skip the DMS data migration. If so, copy the source data to your S3 bucket directly.\nIn today\u0026rsquo;s lab you will copy the data from a centralized S3 bucket to your AWS account, crawl the dataset with an AWS Glue crawler for metadata creation, transform the data with AWS Glue, query the data and create a View with Athena, and finally build a dashboard with Amazon QuickSight.\nOpen AWS CloudShell Open AWS CloudShell . It will open a terminal window in the browser. (If there is a pop-up, close it)\nCopy Data across from staging Amazon S3 bucket to your S3 bucket Issue the following command in the terminal, and replace the bucket name with your own one.\naws s3 cp --recursive --copy-props none s3://aws-dataengineering-day.workshop.aws/data/ s3://\u0026lt;YourBucketName\u0026gt;/tickets/ Verify the Data Open the S3 console and view the data that was copied through CloudShell terminal.\nYour S3 bucket name will look like below: BucketName/bucket_folder_name/schema_name/table_name/objects/\nIn our lab example this becomes: //tickets/dms_sample with a separate path for each table_name\n"
},
{
	"uri": "http://baoandev.github.io/2-realtime/2.1-realtimesetup/",
	"title": "Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink Prelab setup",
	"tags": [],
	"description": "",
	"content": "Introduction This guide will help you set up the pre-lab environment for the Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink lab.\nAfter you deploy the CloudFormation template Kinesis_PreLab.yaml (optionally copy YAML code from the end of this page), sign into your account to view the following resources:\nAfter you deploy the CloudFormation template, sign into your account to view the following resources:\nTwo Amazon Simple Storage Service (Amazon S3) buckets: You will use these buckets to persist raw and processed data. One AWS Lambda function: This Lambda function will be triggered once an anomaly has been detected. Amazon Simple Notification Service (Amazon SNS) topic with an email and phone number subscribed to it: The Lambda function will publish to this topic once an anomaly has been detected. Amazon Cognito User credentials: You will use these user credentials to log into the Kinesis Data Generator to send records to our Amazon Kinesis Data Firehose. CloudFormation Stack Deloyment Click the Deploy to AWS button below to stand up the Kinesis pre-lab workshop infrastructure. https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?stackName=kinesis-pre-lab\u0026templateURL=https://s3.amazonaws.com/aws-dataengineering-day.workshop.aws/Kinesis_PreLab.yaml\nThe button above will open a Quick create stack form: In the Parameters section, fill the following fields as shown in the screenshot: Username: This is your username to login to the Kinesis Data Generator (Alpha-numeric i.e. kinesisUser) Password: This is your password for the Kinesis Data Generator. The password must be at least 6 alpha-numeric characters and contain at least one number and one capital letter. Email: Type an email address that you can access. The SNS topic sends a confirmation to this address. SMS: Type a phone number (+1XXXXXXXXX) where you can receive texts from the SNS topic. In the Capabilities section, select the check box marked \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo;. Click Create. CloudFormation redirects you to your existing stacks. After a few minutes, the kinesis-pre-lab displays a CREATE_COMPLETE status. While the stack runs, watch out for an email like: Confirm the subscription Once your stack is deployed, click the Outputs tab to view more information:\nKinesisDataGeneratorUrl - This value is the Kinesis Data Generator (KDG) URL. RawBucketName - Name of bucket to store the raw data coming from KDG. ProcessedBucketName - Name of the bucket to store transformed data\nCongratulations! You have completed the CloudFormation deployment.\nSet up the Amazon Kinesis Data Generator (KDG) On the Outputs tab, notice the Kinesis Data Generator URL. Navigate to this URL to login into the Amazon Kinesis Data Generator (Amazon KDG).\nThe KDG simplifies the task of generating data and sending it to Amazon Kinesis. The tool provides a user-friendly UI that runs directly in your browser. With the KDG, you can do the following tasks:\nCreate templates that represent records for your specific use cases Populate the templates with fixed data or random data Save the templates for future use Continuously send thousands of records per second to your Amazon Kinesis data stream or Firehose delivery stream Let’s test your Cognito user in the Kinesis Data Generator.\nOn the Outputs tab, click the KinesisDataGeneratorUrl. Sign in using the username and password you entered in the CloudFormation console. After you sign in, you should see the KDG console. You need to set up some templates to mimic the clickstream web payload. Create the following templates but don’t click on Send Data yet, we will do that during the main lab. Copy the tab name highlight in bold letter and value as JSON string, refer screenshot: Schema Discovery Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;DiscoveryKinesisTest\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Click Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Impression Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Verify Email and SMS Subscription On the Amazon SNS navigation menu, select Topics. An SNS topic named ClickStreamEvent2 appears in the display: Click the topic. The Topic details screen appears listing the e-mail/SMS subscription as pending or confirmed. Observe AWS Lambda Anomaly function The CloudFormation template already deployed this Lambda function for you. Spend a few minutes to review the code and understand the actions that trigger the lambda\nIn the console, navigate to AWS Lambda In the AWS Lambda navigation pane, select Functions CSEBeconAnomalyResponse. Click the function to scroll down to the code section. Go through the code in the Lambda code editor. Notice the TopicArn value is your recorded Email/SMS from the subscription step. Lambda will send message to this topic and send notifications. At this point you have all the necessary components to work on the lab.\nThis is the end of the pre-lab, congratulations!\n"
},
{
	"uri": "http://baoandev.github.io/7-gluedatabrew/7.2-gluedatabrew/",
	"title": "Data preparation with Glue DataBrew",
	"tags": [],
	"description": "",
	"content": "Tasks will be completed in this Lab: Create a Glue DataBrew project to explore a dataset Connect a sample dataset from S3 Explore the dataset in Glue DataBrew Generate a rich data profile for the dataset Create a recipe and job to clean and normalize data in the dataset Creating a project Navigate to the AWS Glue DataBrew service\nYou can click on Create project on DataBrew console or select Projects Click Create project\nIn the Project details section, enter covid-states-daily as the project name In the Select a dataset section, select New dataset and enter covid-states-daily-stats In the Connect to a new dataset section, select Amazon S3 under \u0026ldquo;Data lake/data store\u0026rdquo;\nEnter the DatasetS3Path that is available in Workshop Studio Dashboard or outputs section of your CloudFormation stack In the Sampling section, leave the default configuration values In the Permissions section, select the role name like databrew-lab-DataBrewLabRole-xxxxx from the drop-down list. This is the same as specified by the Value of the Key DataBrewLabRole in the CloudFormation Outputs tab. Click Create project Glue DataBrew will create the project, this may take a few minutes. Exploring the dataset When the project has been created, you will be presented with the Grid view. This is the default view, where a sample of the data is shown in tabular format. The Grid view shows\nColumns in the dataset Data type of each column Summary of the range of values that have been found Statistical distribution for numerical columns Click on the Schema tab\nThe Schema view shows the schema that has been inferred from the dataset. In schema view, you can see statistics about the data values in each column.\nIn the Schema view, you can\nSelect the checkbox next to a column to view the summary of statistics for the column values Show/Hide columns Rename columns Change the data type of columns Rearrange the column order by dragging and dropping the columns Click on the Profile tab\nIn the Profile view, you can run a data profile job to examine and collect statistical summaries about the data. A data profile is an assessment in terms of structure, content, relationships, and derivation.\nClick on Run data profile\nIn the Job details and Job run sample panels, leave the default values. In the Job output settings section, select the S3 bucket as specified by the Value of the Key DataBrewOutputS3Bucket in the CloudFormation Outputs tab. It should be with the name like databrew-lab-databrewoutputs3bucket-xxxxx and also specify a prefix name for example data-profile. Include / at the end of the prefix for example /data-profile/ as shown below. In the Permissions section, select the IAM role with the name like databrew-lab-DataBrewLabRole-xxxxx or as specified by the Value of the Key DataBrewLabRole in the CloudFormation Outputs tab.\nLeave all other settings as the default values\nClick Create and run job\nThe data profile job takes approximately 5 minutes complete. Click on Jobs from the menu on the left hand side of the DataBrew console.\nClick on Profile jobs tab to view a list of profile jobs.\nYou can see the status of your profile job on this screen.\nWhen the profile job has successfully completed, click on View data profile This will display the Data profile overview You can also access the data profile from the Profile tab in the project.\nThe data profile shows a summary of the rows and columns in the dataset, how many columns and rows are valid, and correlations between columns.\nClick on the Column statistics tab to view a column-by-column breakdown of the data values. Preparing the dataset In this section, we will apply the following transformations to the dataset.\nConvert the date column from integer to string Split the date column into three new columns (year, month, day) to partition the data by these columns Fill the missing values in the probableCases column with 0 Map the values of the dataQualityGrade column to a numerical value Navigate back to the covid-states-daily project grid view by clicking on PROJECTS to get there.\nDataBrew has inferred data type of the date column as integer. We will convert the data type of the date column to string.\nClick on the # icon next to the date column name and select string Click on Apply at the far right. Note that the transformation is added to the recipe at the right. We will duplicate the date column first before splitting it into year, month, day columns, as the original column will be deleted by this transformation.\nSelect the \u0026hellip; at the top of the date column.\nFrom the pop-up menu, scroll to the bottom and select Duplicate Leave the default settings in the Duplicate column dialog Click Apply\nA copy of the date column is created with the name date_copy. Note that the duplicate column transformation is added as a step to the recipe at the right.\nLet\u0026rsquo;s split the date_copy column into year, month, day columns.\nSelect the \u0026hellip; at the top of the date_copy column.\nSelect Split column \u0026ndash;\u0026gt; At positions from beginning In the Split column dialog, enter 4 for Position from the beginning to split out the year. Leave all other default settings. In the Split column dialog, scroll down and click Preview changes to see how the column is split. Note that the date_copy column is marked for deletion. Click Apply\nNext, split the date_copy_2 column into month and day.\nSame way as you did above in (4). In the Split column dialog, enter 2 for Position from the beginning to split out the month, and day. Leave all other default settings.\nThe result should look like the screenshot below. Let\u0026rsquo;s rename the new columns to year, month, day.\nClick on Rename from the \u0026hellip; at the top right of the date_copy_1 column. Enter year as the new column name, and click Apply Rename the other two new columns - date_copy_2_1 and date_copy_2_2 - to month and day respectively.\nThe result should look like the following. The probableCases column has some missing values. We will set these missing values to 0.\nTo navigate to the probableCases column, click on the Columns drop-down list at the top, enter probableCases in the search field and click View. Click on the \u0026hellip; from the top right of probableCases column and select Remove or fill missing values \u0026ndash;\u0026gt; Fill with custom value Enter 0 as the Custom value and click Apply Map the values of the dataQualityGrade column to numerical values.\nTo navigate to the dataQualityGrade column, click on the columns drop-down list at the top, enter dataQualityGrade in the search field and click View. Click on the \u0026hellip; from the top right of dataQualityGrade column and select Categorical mapping In the Categorically map column dialog\nSelect the option Map all values\nSelect Map values to numeric values\nMap the current dataQualityGrade value to the new value as follows Leave all other settings as default. Click Apply\nAfter this transform, the new column dataQualityGrade_mapped is of type double, convert this column to integer. By clicking on the # on top left of the new column dataQualityGrade_mapped. Click on Apply on the far right to confirm changes.\nYou are now ready to publish the recipe so that it can be used in DataBrew jobs. The final recipe looks like the following. Click on the Publish button at the top of the recipe.\nOptionally enter a version description, and click Publish The recipe is published as Version 1.0. DataBrew applies a version number when a recipe is published.\nCreating a DataBrew job Click on Jobs from the menu on the left hand side of the DataBrew console.\nOn the Recipe jobs tab, click on Create job\nEnter covid-states-daily-prep for the job name\nSelect Create a recipe job\nSelect the covid-states-daily-stats dataset\nSelect the covid-states-daily-recipe. In the Job output settings section set\nOutput to Amazon S3. File type to CSV. Delimiter to Comma(,). Compression to None. S3 bucket owners\u0026rsquo;s Account to Current AWS account. S3 location as specified by the Value of the Key DataBrewOutputS3Bucket in the CloudFormation Outputs tab. It should be with the name like databrew-lab-databrewoutputs3bucket-xxxxx and also specify a prefix name for example job-outputs. The S3 location should look something like s3://databrew-lab-databrewoutputs3bucket-xxxxx/job-outputs/. Click on Settings on the right to\nSelect Create a new folder for each job run.\nUnder Custom partition by column values in the File partitioning section click on Enabled. Then search and Add year, month and day as Columns to partition by. This will partition the data in the output folder by year, month and day. And click on Save.\nIn the Permissions section, select the IAM role with the name like databrew-lab-DataBrewLabRole-xxxxx or as specified by the Value of the Key DataBrewLabRole in the CloudFormation Outputs tab. Click Create and run job\nThe DataBrew job is created and the job status is Running Wait until the job has completed successfully (approx. 4 minutes) Click on the link to the job output, and verify that the output files are partitioned in the S3 bucket\nViewing data lineage In DataBrew, navigate back to the covid-states-daily project. Click on Lineage at the top right. This view shows the origin of the data and the transformation steps that the data has been through. Congratulations, you have completed the DataBrew lab. "
},
{
	"uri": "http://baoandev.github.io/2-realtime/2.2-lab/",
	"title": "Lab: Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Introduction This guide helps you complete Real-Time Clickstream Anomaly Detection using Amazon Managed Service for Apache Flink.\nAnalyzing web log traffic to gain insights that drive business decisions has historically been performed using batch processing. Although effective, this approach results in delayed responses to emerging trends and user activities. There are solutions that process data in real-time using streaming and micro-batching technologies, but they can be complex to set up and maintain. Amazon Managed Service for Apache Flink is a managed service that makes it easy to identify and respond to changes in data behavior in real-time.\nSteps Set up an Amazon Analytics Studio Application through CloudFormation stack deployment Generate real time website traffic using Amazon Kinesis Data Generator (KDG) Perform real-time Data Analytics Environment Cleanup Appendix: Anomaly Detection Scripts In the Kinesis prelab setup, you fulfilled the prerequisites for this lab. In this lab, you will create an Amazon Managed Service for Apache Flink pipeline. Set up an Amazon Analytics Studio Application through CloudFormation stack deployment Click the Deploy to AWS link here to stand up the pre-lab workshop infrastructure https://console.aws.amazon.com/cloudformation/home?#/stacks/quickcreate?templateURL=https://aws-bigdata-blog.s3.amazonaws.com/DE-ID-KDA-Lab/1-Lab-Kinesis-Clickstream_CFN.yaml\u0026stackName=kda-flink-pre-lab\u0026param_FlinkVersion=1.15.4\u0026param_Release=master\u0026param_GlueDatabaseName=prelab_kda_db\nThe link above will open a “Quick create stack” form, please accept the default parameters, select the checkbox to acknowledge new IAM role creation and select Create Stack to run the Amazon CloudFormation Template The stack will create six Amazon Kinesis Data Streams in the Amazon Kinesis Console\na. tickerstream – the raw stream to send the initial traffic\nb. clickstream – captures the number of clicks\nc. impressionstream – captures the number of impressions\nd. ctrstream – captures the calculated click through rate\ne. destinationstream – captures the anomaly scores\nf. anomalydetectionstream – captures the records with anomaly score greater than 2 The template would also create an Amazon Analytics Studio application called kda-flink-prelab-RealtimeApplicationNotebook in the Amazon Kinesis Application Console → Studio tab. We will write interactive Studio Notebook in Apache Zeppelin for real time data analysis. Run the Studio Application by selecting the kda-flink-prelab-RealtimeApplicationNotebook under Studio tab. Select “Run” again on the next screen. Generate real time website traffic using Amazon Kinesis Data Generator (KDG) Navigate to the Amazon CloudFormation console in your AWS account, click on the Kinesis-pre-lab stack created during the Streaming Data Analytics Prelab setup .\nGo to the Outputs tab of the stack to get the Kinesis Data Generator link as shown below\nOpen two concurrent sessions of the KDG UI in your browser. Sign in using the username and password you entered in the CloudFormation template while creating the Streaming Data Analytics Prelab setup\na. We want to generate more Click messages than Impressions. So, in the first session, send impression messages at rate of one message per second for 30 seconds to the tickerstream, the message body is\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;https://www.mysite.com\u0026rdquo;} b. Then in the second session, send click messages at a rate of 5 messages per second for 30 seconds to the tickerstream, the message body is\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;https://www.mysite.com\u0026rdquo;} c. You can view the number of messages being sent to the data stream when you start posting the messages d. After 30 seconds, please stop sending both Click and Impression messages.\nPerform real-time Data Analytics Navigate to the Amazon Kinesis Application Console . Under the Studio tab, select kda-prelab-template-RealtimeApplicationNotebook. Select “Open in Apache Zeppelin”. On the Apache Zeppelin Console, select Create new note. Provide the notebook name as kda_interactive_notebook Now lets perform real time interactive analytics with Kinesis data streams. We will\ni. Create Flink tables using Flink SQL Queries\nii. Use Flink SQL queries to transform and create new data streams in real-time\niii. Perform anomaly detection using Flink User Defined Function and trigger anomaly notification emails in real-time.\niv. The scripts are available here.\nhttps://docs.google.com/document/d/1PUBiZUpPkydsek_Navj8f3mskMqE8e_6mjp_rglkq28/edit?usp=sharing\nv. A notebook is also available here which can be downloaded and imported through Apache Zeppelin console. vi. You can then open the notebook and run the paragraphs one after the other.\nvii. Please refer to the anomaly detection logic in the image below before you start running the remaining steps Run the table creation scripts. User Defined Function (UDF) performs Anomaly Detection in real-time using Random Cut Forest algorithm. Run step #2. You can view real time data from website visits by running the query in Step #3. Create impressionstream by filtering messages from tickerstream. Create clickstream by filtering messages from tickerstream. Calculate Click Through Rate (CTR) and populate ctrstream. You can view the Click Through Rate in real time by executing Step #7. Use the UDF (Random Cut Forest) to generate anomaly scores. Populate anomalydetectionstream by executing Step #9. Now check the anomaly scores from Random Cut Forest algorithm in real time. You will start receiving notifications in your email when anomalies are detected: If you do not receive the anomaly notification emails on your first attempt\ni. Open the two concurrent sessions of KDG UI in your browser again.\nii. In the first session, send impression messages at rate of one message per second to the tickerstream, the message body is\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;:\u0026ldquo;https://www.mysite.com\u0026rdquo;}\niii. In the second session, send click messages at a rate of five messages per second to the tickerstream, the message body is\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;:\u0026ldquo;https://www.mysite.com\u0026rdquo;}\niv. Stop sending messages after 30-40 seconds.\nv. Now on the Apache Zeppelin Notebook, repeat steps 3 to 10 and you should start receiving email notifications from the second attempt.\nEnvironment Clean Up After completing the lab, click Actions → Stop Application to stop your application and avoid flood of SMS and e-mails messages.\nIf you would like to delete the entire resource stack, navigate to Amazon CloudFormation Console → Stacks, select kda-flink-pre-lab and click Delete to remove the stack. This step will clean up all the resources created by the stack earlier. Select Delete on the next screen "
},
{
	"uri": "http://baoandev.github.io/2-realtime/",
	"title": "Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink ",
	"tags": [],
	"description": "",
	"content": "Introduction Streaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.\nThis data needs to be processed sequentially and incrementally on a record-by-record basis or over sliding time windows, and used for a wide variety of analytics including correlations, aggregations, filtering, and sampling. Information derived from such analysis gives companies visibility into many aspects of their business and customer activity such as –service usage (for metering/billing), server activity, website clicks, and geo-location of devices, people, and physical goods –and enables them to respond promptly to emerging situations. For example, businesses can track changes in public sentiment on their brands and products by continuously analyzing social media streams, and respond in a timely fashion as the necessity arises.\nAWS has a very robust stack of technologies such as Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Service for Apache Flink and Managed Streaming for Kafka when it comes to working with streaming data. In this lab, we will cover some of these key services with hands on exercises.\nContent Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink Prelab setup Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink Streaming ETL with Glue "
},
{
	"uri": "http://baoandev.github.io/3-ingestion/",
	"title": "Ingestion with DMS",
	"tags": [],
	"description": "",
	"content": "Introduction AWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.\nAWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. With AWS Database Migration Service, you can also continuously replicate data with low latency from any supported source to any supported target. For example, you can replicate from multiple sources to Amazon Simple Storage Service (Amazon S3) to build a highly available and scalable data lake solution. You can also consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. Learn more about the supported source and target databases.\nLab tasks will cover the following navigation excercise DMS lab has three options:\nIf you want to have a dive deep hands-on experience in DMS (Data Migration Service) then first run the instructor lab to mimic an on-prem relational database environment, followed by the student lab to create the required data-migration infrastructure in AWS. The main lab will help you perform actual data migration from a relational database to a datalake in AWS. If you want to start with Glue ETL and skip the DMS hands-on altogether, please run the DMS auto-complete lab. Autocomplete may take between 15 to 20 minutes to make available all the data in a centralize S3 data lake raw layer and you will be ready to dive-deep on data transformation using Glue ETL. If you don\u0026rsquo;t want to go through the DMS service lab, can directly copy the raw data to S3 by the Option 3: Skip DMS Lab. The limitation of this option is you can\u0026rsquo;t use DMS to generate the incremental data to test Glue\u0026rsquo;s bookmark feature. "
},
{
	"uri": "http://baoandev.github.io/2-realtime/2.3-labglue/",
	"title": "Lab: Streaming ETL with Glue",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab you will learn how to ingest, process, and consume streaming data using AWS serverless services such as Kinesis Data Streams, Glue, S3, and Athena. To simulate the data streaming input, we will use Kinesis Data Generator (KDG). We have explained KDG earlier in the Streaming Data Analytics Prelab setup page.\nStreaming ETL Lab Set up the environment Create CloudFormation with the link belown: https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?stackName=dmslab-student\u0026templateURL=https://s3.amazonaws.com/aws-dataengineering-day.workshop.aws/DMSlab_student_CFN.yaml Set up the kinesis stream Navigate to AWS Kinesis console\nClick “Create data stream” Enter the following details\nData stream name: TicketTransactionStreamingData Capacity mode: Provisioned Provisioned shards: 2 Click Create data stream\nLab Instructions Create Table for Kinesis Stream Source in Glue Data Catalog Navigate to AWS Glue console\nOn the AWS Glue menu, select Data Catalog and then Tables, then click the Add table button. Enter TicketTransactionStreamData as the table name\nClick Create database and enter tickettransactiondatabase as the database name, and click create. Use the drop down to select the database we just created, and click Next. Select Kinesis as the source, select Stream in my account to select a Kinesis data stream, select the appropriate AWS region where you have created the stream, select the stream name as TicketTransactionStreamingData from the dropdown, choose JSON as the incoming data format, as we will be sending JSON payloads from the Kinesis Data Generator in the following steps. and click Next. Leave the schema as empty, as we will enable the schema detection feature when defining the Glue stream job. Leave partition indices empty. Click Next. Review all the details and click Create. Create and trigger the Glue Streaming job On the left-hand side of the Glue Console, click on Data Integration and ETL and click on ETL jobs. Click the Create job from a blank graph button. For the source, click Amazon Kinesis from the list. Click on the \u0026ldquo;Amazon Kinesis\u0026rdquo; node on the canvas, then in the panel on the right under “Data source properties - Kinesis Stream”, configure as follows: ** Name** Amazon Kinesis\nAmazon Kinesis Source: Data Catalog table\nDatabase: tickettransactiondatabase\nTable: tickettransactionstreamdata\nMake sure that Detect schema is selected\nLeave all other fields as default Click on the \u0026ldquo;+\u0026rdquo; button to add nodes. Then click \u0026ldquo;Targets\u0026rdquo; tab and click Amazon S3. Click the Data target - S3 bucket on the canvas, then in the panel on the right under “Data target properties - S3”, configure as follows:\nFormat: Parquet Compression Type: Uncompressed S3 Target Location: Select Browse S3 and select the “xxx-xxx-dmslabs3bucket-xxx” bucket Finally, select the Job details tab at the top and configure as follows: Name: TicketTransactionStreamingJob IAM Role: Select the xxx-GlueLabRole-xxx from the drop down list Type: Spark Streaming Press the Save button in the top right-hand corner to create the job.\nOnce you see the Successfully created job message in the banner, click the Run button to start the job.\nTrigger streaming data from KDG Launch KDG using the url you bookmarked from the lab setup. Login using the user and password you provided when deploying the Cloudformation stack. Make sure you select the appropriate region, from the dropdown list, select the TicketTransactionStreamingData as the target Kinesis stream, leave Records per second as the default (100 records per second); for the record template, type in NormalTransaction as the payload name, and copy the template payload as follows: { \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click Send data to trigger the simulated transaction streaming data. "
},
{
	"uri": "http://baoandev.github.io/4-transformingdatawithglue/",
	"title": "Transforming data with Glue",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab you will learn about AWS Glue, which is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development. You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables.\n"
},
{
	"uri": "http://baoandev.github.io/5-queryandvisualize/",
	"title": "Query and Visualize",
	"tags": [],
	"description": "",
	"content": "Introduction This lab introduces you to AWS Glue, Amazon Athena, and Amazon QuickSight. AWS Glue is a fully managed data catalog and ETL service; Amazon Athena provides the ability to run ad-hoc queries on your data in your data lake; and Amazon QuickSight provides visualization of the data you import.\n"
},
{
	"uri": "http://baoandev.github.io/7-gluedatabrew/",
	"title": "Glue DataBrew",
	"tags": [],
	"description": "",
	"content": " DataBrew Prelab Data preparation with Glue DataBrew "
},
{
	"uri": "http://baoandev.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://baoandev.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]