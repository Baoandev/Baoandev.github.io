[
{
	"uri": "http://baoandev.github.io/vi/5-queryandvisualize/5.1-athenaandquicksight/",
	"title": "Athena and Quicksight",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://baoandev.github.io/vi/2-realtime/2.1-realtimesetup/",
	"title": "Cài đặt trước phòng thí nghiệm phát hiện bất thường trong luồng nhấp chuột theo thời gian thực với Dịch vụ Quản lý Amazon cho Apache Flink",
	"tags": [],
	"description": "",
	"content": "Introduction This guide will help you set up the pre-lab environment for the Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink lab.\nAfter you deploy the CloudFormation template Kinesis_PreLab.yaml (optionally copy YAML code from the end of this page), sign into your account to view the following resources:\nSau khi bạn triển khai mẫu CloudFormation, đăng nhập vào tài khoản của bạn để xem các tài nguyên sau:\nHai bucket Amazon Simple Storage Service (Amazon S3): Bạn sẽ sử dụng các bucket này để lưu trữ dữ liệu thô và dữ liệu đã xử lý. Một hàm AWS Lambda: Hàm Lambda này sẽ được kích hoạt khi phát hiện ra một bất thường. Chủ đề Amazon Simple Notification Service (Amazon SNS) với email và số điện thoại đã đăng ký: Hàm Lambda sẽ đăng bài lên chủ đề này khi phát hiện ra một bất thường. Thông tin đăng nhập Người dùng Amazon Cognito: Bạn sẽ sử dụng các thông tin đăng nhập này để đăng nhập vào Kinesis Data Generator để gửi bản ghi đến Amazon Kinesis Data Firehose. Triển khai CloudFormation Stack Nhấn nút Deploy to AWS bên dưới để thiết lập cơ sở hạ tầng cho hội thảo trước phòng lab Kinesis. https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?stackName=kinesis-pre-lab\u0026templateURL=https://s3.amazonaws.com/aws-dataengineering-day.workshop.aws/Kinesis_PreLab.yaml\nNút bên trên sẽ mở một biểu mẫu Quick create stack: Trong phần Parameters, điền các trường sau như trong ảnh chụp màn hình: Username: Đây là tên người dùng của bạn để đăng nhập vào Kinesis Data Generator (Chữ và số, ví dụ: kinesisUser) Password: Đây là mật khẩu của bạn cho Kinesis Data Generator. Mật khẩu phải có ít nhất 6 ký tự bao gồm chữ và số, chứa ít nhất một chữ số và một chữ cái viết hoa. Email: Nhập một địa chỉ email mà bạn có thể truy cập. Chủ đề SNS sẽ gửi xác nhận đến địa chỉ này. SMS: Nhập một số điện thoại (+1XXXXXXXXX) nơi bạn có thể nhận tin nhắn từ chủ đề SNS. Trong phần Capabilities, chọn hộp kiểm \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo;. Nhấn Create. CloudFormation sẽ chuyển hướng bạn đến các stack hiện có. Sau vài phút, stack kinesis-pre-lab hiển thị trạng thái CREATE_COMPLETE. Trong khi stack chạy, chú ý tới email như: Xác nhận đăng ký Sau khi stack của bạn được triển khai, nhấn tab Outputs để xem thêm thông tin:\nKinesisDataGeneratorUrl - Giá trị này là URL của Kinesis Data Generator (KDG). RawBucketName - Tên của bucket để lưu trữ dữ liệu thô từ KDG. ProcessedBucketName - Tên của bucket để lưu trữ dữ liệu đã chuyển đổi. Chúc mừng! Bạn đã hoàn thành việc triển khai CloudFormation.\nThiết lập Amazon Kinesis Data Generator (KDG) Trên tab Outputs, chú ý đến URL Kinesis Data Generator. Điều hướng đến URL này để đăng nhập vào Amazon Kinesis Data Generator (Amazon KDG).\nKDG đơn giản hóa việc tạo dữ liệu và gửi đến Amazon Kinesis. Công cụ cung cấp một giao diện người dùng dễ sử dụng chạy trực tiếp trong trình duyệt của bạn. Với KDG, bạn có thể thực hiện các tác vụ sau:\nTạo mẫu đại diện cho các bản ghi cho các trường hợp sử dụng cụ thể của bạn Điền các mẫu với dữ liệu cố định hoặc dữ liệu ngẫu nhiên Lưu các mẫu để sử dụng trong tương lai Liên tục gửi hàng nghìn bản ghi mỗi giây đến luồng dữ liệu Amazon Kinesis hoặc luồng phân phối Firehose Hãy kiểm tra người dùng Cognito của bạn trong Kinesis Data Generator.\nTrên tab Outputs, nhấn vào KinesisDataGeneratorUrl. Đăng nhập bằng tên người dùng và mật khẩu bạn đã nhập trong bảng điều khiển CloudFormation. Sau khi đăng nhập, bạn sẽ thấy bảng điều khiển KDG. Bạn cần thiết lập một số mẫu để mô phỏng tải trọng clickstream web. Tạo các mẫu sau nhưng đừng nhấn Send Data vội, chúng ta sẽ thực hiện điều đó trong phòng lab chính. Sao chép tên tab được in đậm và giá trị dưới dạng chuỗi JSON, tham khảo ảnh chụp màn hình: Schema Discovery Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;DiscoveryKinesisTest\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Click Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Impression Payload : {\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;yourwebsiteurl.domain.com\u0026rdquo;} Xác minh Đăng ký Email và SMS Trên menu điều hướng Amazon SNS, chọn Topics. Một chủ đề SNS tên là ClickStreamEvent2 sẽ xuất hiện: Nhấp vào chủ đề. Màn hình chi tiết chủ đề sẽ liệt kê đăng ký e-mail/SMS là đang chờ xử lý hoặc đã xác nhận. Quan sát hàm AWS Lambda Anomaly Mẫu CloudFormation đã triển khai hàm Lambda này cho bạn. Dành vài phút để xem xét mã và hiểu các hành động kích hoạt lambda.\nTrong bảng điều khiển, điều hướng đến AWS Lambda Trong ngăn điều hướng AWS Lambda, chọn Functions CSEBeconAnomalyResponse. Nhấp vào chức năng để cuộn xuống phần mã. Đi qua mã trong trình soạn thảo mã Lambda. Chú ý giá trị TopicArn là email/số điện thoại đã ghi nhận từ bước đăng ký. Lambda sẽ gửi thông báo đến chủ đề này và gửi thông báo. Đến đây, bạn đã có tất cả các thành phần cần thiết để làm việc trên phòng lab.\nĐây là kết thúc của phần thiết lập trước phòng lab, chúc mừng bạn!\n"
},
{
	"uri": "http://baoandev.github.io/vi/4-transformingdatawithglue/4.1-datavalidationandetl/",
	"title": "Data Validation and ETL",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Crawler để tải toàn bộ dữ liệu ban đầu Truy cập vào AWS Glue Console.\nTrên menu AWS Glue, dưới mục \u0026lsquo;Data Catalog\u0026rsquo;, chọn Crawlers.\nNhấn Create crawler.\nNhập glue-lab-crawler làm tên crawler cho lần tải dữ liệu ban đầu.\nTùy chọn: Nhập mô tả. Mô tả này nên rõ ràng và dễ nhận biết, sau đó nhấn Next.\nDưới mục Choose data sources and classifiers, chọn Not yet và nhấn Add a data source.\nTrên trang Add a data store, thực hiện các lựa chọn sau:\nĐối với Data source, nhấn vào hộp thả xuống và chọn S3. Đối với Location of S3 data, chọn In this account. Đối với S3 path, duyệt đến thư mục đích lưu trữ tệp CSV, ví dụ: s3://xxx-dmslabs3bucket-xxx/tickets/. Để các tham số khác ở chế độ mặc định. Nhấn Add an S3 data source. Nhấn Next.\nTrên trang Configure security settings, chọn:\nDưới mục Existing IAM role, chọn \u0026lt;stackname\u0026gt;-GlueLabRole-\u0026lt;RandomString\u0026gt; đã được tạo sẵn cho bạn. Ví dụ: “mod-3fccddd123456789-GlueLabRole-ZOQDII7JTBUM”. Nhấn Next.\nTrên trang Set output and scheduling, dưới mục Target database, nhấn Add database để mở một tab khác.\nNhập ticketdata làm tên cơ sở dữ liệu và nhấn Create database.\nQuay lại trang Set output and scheduling:\nChọn Target database là ticketdata. Nhấn nút làm mới ngay cạnh danh sách thả xuống để đảm bảo cơ sở dữ liệu mới tạo xuất hiện. Đối với Prefix added to tables (optional), để trống. Dưới mục Crawler schedule, chọn tần suất là On demand. Nhấn Next.\nXem lại thông tin và nhấn Create crawler. Crawler bây giờ đã sẵn sàng để chạy.\nChạy Crawler bằng cách nhấn vào nút Run crawler.\nCrawler sẽ thay đổi trạng thái từ Running sang Stopping, đợi đến khi crawler trở về trạng thái Ready (quá trình này sẽ mất vài phút). Bạn có thể thấy rằng nó đã tạo ra 15 bảng.\nTrong bảng điều khiển điều hướng AWS Glue, dưới mục Data Catalog, nhấn Databases → Tables. Bạn cũng có thể nhấn vào cơ sở dữ liệu ticketdata để duyệt các bảng.\nBài tập Xác thực Dữ liệu Trong phần Tables của cơ sở dữ liệu ticketdata, nhấn vào bảng person. Bạn có thể nhận thấy rằng một số bảng (như bảng person) có tiêu đề cột là col0, col1, col2, col3. Khi không có tiêu đề hoặc crawler không thể xác định loại tiêu đề, các tiêu đề cột mặc định sẽ được chỉ định.\nBài tập này sử dụng bảng person để minh họa cách giải quyết vấn đề này.\nNhấn vào tùy chọn Edit Schema. Trong phần Edit Schema, chọn hàng cho col0 (tên cột) và nhấn Edit. Nhập id làm tên cột và nhấn Save.\nLặp lại các bước trên để đổi tên các cột còn lại thành full_name, last_name, và first_name theo hình minh họa sau: Nhấn Save as new table version. Bài tập ETL Dữ liệu Điều kiện tiên quyết: Để lưu trữ dữ liệu đã xử lý ở định dạng parquet, bạn cần tạo một vị trí thư mục mới cho mỗi bảng, ví dụ, đường dẫn đầy đủ cho bảng sport_team sẽ là s3://\u0026lt;s3_bucket_name\u0026gt;/tickets/dms_parquet/sport_team.\nGlue sẽ tự động tạo thư mục mới dựa trên đường dẫn đầy đủ mà bạn nhập, như ví dụ trên. Vui lòng tham khảo hướng dẫn sử dụng để biết cách tạo thư mục thủ công trong bucket S3.\nTrong bảng điều hướng bên trái, nhấn vào ETL jobs.\nNhấn vào “Visual ETL”.\nChọn Amazon S3 từ danh sách Sources để thêm một nút Data source - S3 bucket.\nChọn nút Data source - S3 bucket để hiển thị các thuộc tính của nguồn dữ liệu.\nTrong bảng điều khiển bên phải, dưới mục Data source properties - S3, chọn Data Catalog table và chọn cơ sở dữ liệu ticketdata từ danh sách thả xuống. Đối với Table, chọn bảng sport_team.\nNhấn vào nút + và chọn Change Schema từ danh sách Transforms để thêm một nút Transform - Change Schema.\nChọn nút Transform - Change Schema để hiển thị các thuộc tính. Trong bảng Transform bên phải, thay đổi kiểu dữ liệu của cột id thành double trong danh sách thả xuống.\nNhấn vào nút + và chọn Amazon S3 từ danh sách Targets để thêm một nút Data target - S3 bucket.\nChọn nút Data target - S3 bucket để hiển thị các thuộc tính. Trong bảng điều khiển bên phải, thay đổi Format thành Parquet trong danh sách thả xuống. Dưới Compression Type, chọn Uncompressed từ danh sách thả xuống.\nDưới mục S3 Target Location, chọn Browse S3, duyệt đến bucket mod-xxx-dmslabs3bucket-xxx, chọn mục tickets và nhấn Choose.\nTrong hộp văn bản, thêm dms_parquet/sport_team/ vào cuối URL S3. Đường dẫn nên giống với s3://mod-xxx-dmslabs3bucket-xxx/tickets/dms_parquet/sport_team/ - đừng quên dấu / ở cuối. Công việc này sẽ tự động tạo thư mục.\nCuối cùng, chọn tab Job details ở trên cùng. Nhập Glue-Lab-SportTeamParquet vào trường Name.\nĐối với IAM Role, chọn vai trò có tên tương tự mod-xxx-GlueLabRole-xxx.\nCuộn xuống dưới trang và trong mục Job bookmark, chọn Disable từ danh sách thả xuống. Bạn có thể thử tính năng bookmark sau trong bài lab này.\nNhấn nút Save ở góc trên bên phải để tạo công việc.\nKhi bạn thấy thông báo Successfully created job trên banner, nhấn nút Run để bắt đầu công việc.\nChọn Jobs từ bảng điều hướng bên trái để xem danh sách các công việc của bạn.\nChọn Monitoring từ bảng điều hướng bên trái để xem các công việc đang chạy, tỷ lệ thành công/thất bại và các số liệu thống kê khác.\nCuộn xuống danh sách Job runs để xác minh rằng công việc ETL đã hoàn thành thành công. Quá trình này sẽ mất khoảng 1 phút để hoàn thành.\nChúng ta cần lặp lại quy trình này cho 4 công việc khác, để chuyển đổi các bảng sport_location, sporting_event, sporting_event_ticket, và person.\nTrong quá trình này, chúng ta sẽ cần phải chỉnh sửa các kiểu dữ liệu cột khác nhau. Chúng ta có thể lặp lại quy trình ở trên cho mỗi bảng, hoặc có thể sao chép công việc đầu tiên và cập nhật các chi tiết. Các bước dưới đây mô tả cách sao chép công việc - nếu bạn tạo thủ công mỗi lần, hãy làm theo các bước trên nhưng đảm bảo bạn sử dụng các giá trị được cập nhật từ các bảng dưới đây.\nQuay lại menu Jobs, và chọn công việc Glue-Lab-SportsTeamParquet bằng cách nhấp vào ô checkbox bên cạnh tên.\nDưới mục Actions trong danh sách thả xuống, chọn Clone job. Cập nhật công việc theo các bảng dưới đây, sau đó nhấn Save và Run.\nSport_Location:\nTạo công việc Glue-Lab-SportLocationParquet với các thuộc tính sau:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sport_location Transform - ApplyMapping node Schema transformations None Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sport_location/ Job details tab Job Name Glue-Lab-SportLocationParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Sporting_Event:\nTạo công việc Glue-Lab-SportingEventParquet với các thuộc tính sau:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sporting_event Transform - ApplyMapping node Schema transformations column “start_date_time” =\u0026gt; TIMESTAMP column “start_date” =\u0026gt; DATE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sporting_event/ Job details tab Job Name Glue-Lab-SportingEventParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Sporting_Event_Ticket:\nTạo công việc Glue-Lab-SportingEventTicketParquet với các thuộc tính sau:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table sporting_event_ticket Transform - ApplyMapping node Schema transformations column “id” =\u0026gt; DOUBLE column “sporting_event_id” =\u0026gt; DOUBLE column “ticketholder_id” =\u0026gt; DOUBLE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/sporting_event_ticket/ Job details tab Job Name Glue-Lab-SportingEventTicketParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Person:\nTạo công việc Glue-Lab-PersonParquet với các thuộc tính sau:\nTask / Action Attribute Values Data source - S3 bucket node Database ticketdata Table person Transform - ApplyMapping node Schema transformations column “id” =\u0026gt; DOUBLE Data target - S3 bucket node Format Parquet Compression Type Uncompressed S3 target path tickets/dms_parquet/person/ Job details tab Job Name Glue-Lab-PersonParquet IAM Role xxx-GlueLabRole-xxx Job bookmark Disable Tạo Glue Crawler cho Các Tệp Parquet Trong menu điều hướng Glue, dưới mục Data Catalog chọn Crawlers. Nhấn Create crawler.\nĐối với Crawler name, nhập glue-lab-parquet-crawler và nhấn Next.\nTrong màn hình Add data store:\nĐối với Choose a data store, chọn S3. Đối với Location of S3 data, chọn In this account. Đối với S3 path, chỉ định đường dẫn S3 (Thư mục Parquet chính) chứa các tệp parquet lồng ghép, ví dụ: s3://xxx-dmslabs3bucket-xxx/tickets/dms_parquet/ Giữ tất cả các tham số khác ở mặc định. Nhấn Add an S3 data source.\nTrong màn hình tiếp theo Choose data sources and classifiers, nhấn Next.\nTrên trang Configure security settings, chọn Choose an existing IAM role.\nĐối với IAM role, chọn vai trò đã tồn tại có tên giống như xxx-GlueLabRole-xxx và Nhấn Next.\nTrên trang Set output and scheduling,\nDưới mục Target database, chọn cơ sở dữ liệu bạn đã tạo trước đó, ví dụ: ticketdata. Đối với Prefix added to tables (optional), nhập parquet_. Đối với Crawler schedule, chọn On Demand và nhấn Next.\nXem lại trang tổng kết và nhấn Create crawler.\nNhấn Run Crawler. Khi crawler của bạn đã hoàn tất, bạn sẽ thấy các bảng đã được thêm vào, tùy thuộc vào số lượng chuyển đổi ETL parquet mà bạn đã thiết lập ở phần trước.\nXác nhận bạn có thể thấy các bảng Trong bảng điều hướng bên trái, nhấn vào Tables.\nThêm bộ lọc parquet cho Classification để hiển thị các bảng mới được tạo.\n"
},
{
	"uri": "http://baoandev.github.io/vi/7-gluedatabrew/7.1-prelab/",
	"title": "DataBrew Prelab",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong bài thực hành này, chúng ta sẽ sử dụng AWS Glue DataBrew để khám phá một tập dữ liệu trong S3, làm sạch và chuẩn bị dữ liệu.\nĐể thực hiện điều này, đầu tiên chúng ta sẽ thiết lập một vai trò IAM để sử dụng trong DataBrew và một bucket S3 cho kết quả từ các công việc của DataBrew.\nTriển khai CloudFormation Stack Nhấn vào liên kết Deploy to AWS bên dưới để tạo các tài nguyên AWS cho bài thực hành DataBrew. https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?templateURL=https://s3.us-east-1.amazonaws.com/aws-dataengineering-day.workshop.aws/DataBrew_PreLab_CFN.yaml\u0026stackName=databrew-lab\u0026param_SourceBucket=aws-dataengineering-day.workshop.aws\u0026param_SourceKey=states_daily.csv.gz\nĐánh dấu vào ô \u0026ldquo;I acknowledge that \u0026hellip;\u0026rdquo;, sau đó nhấn vào \u0026ldquo;Create Stack\u0026rdquo; để tạo stack. Việc này sẽ mất khoảng một phút để hoàn tất. Sau khi stack của bạn được triển khai, nhấn vào tab Outputs để xem thêm thông tin. Ghi chú các giá trị của DatasetS3Path, DataBrewLabRole và DataBrewOutputS3Bucket sẽ được sử dụng trong bài thực hành.\nChúc mừng! Bạn đã hoàn thành việc triển khai CloudFormation.\n"
},
{
	"uri": "http://baoandev.github.io/vi/",
	"title": "Dữ liệu cùng AWS ",
	"tags": [],
	"description": "",
	"content": "Ngày Đắm Chìm vào Kỹ Thuật Dữ Liệu Ngày Đắm Chìm vào Kỹ Thuật Dữ Liệu là gì? Ngày Đắm Chìm vào Kỹ Thuật Dữ Liệu có các phòng lab và mô-đun thực hành tập trung vào việc nạp dữ liệu, cấp nước, khám phá và tiêu thụ dữ liệu trong một hồ dữ liệu trên AWS.\nLợi ích của Ngày Đắm Chìm vào Kỹ Thuật Dữ Liệu Ngày Đắm Chìm vào Kỹ Thuật Dữ Liệu cung cấp thời gian thực hành với các dịch vụ phân tích AWS bao gồm Amazon Kinesis Services cho việc nạp và phân tích dữ liệu theo luồng, dịch vụ Di chuyển Dữ liệu AWS cho việc nạp dữ liệu theo lô, AWS Glue cho việc lập chỉ mục dữ liệu và thực hiện ETL trên hồ dữ liệu, Amazon Athena để truy vấn hồ dữ liệu, và Amazon Quicksight cho việc trực quan hóa. Ngày Đắm Chìm này giúp xây dựng một hồ dữ liệu serverless phù hợp với tương lai và dựa trên đám mây.\nNội dung Giới thiệu Phát hiện Anomalies Clickstream theo thời gian thực với Amazon Managed Service cho Apache Flink Nạp dữ liệu với DMS Chuyển đổi dữ liệu với Glue Truy vấn và Trực quan hóa Glue DataBrew "
},
{
	"uri": "http://baoandev.github.io/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Những gì bạn sẽ thực hiện trong các phòng lab này: Các phòng lab này được thiết kế để hoàn thành theo trình tự, và toàn bộ hướng dẫn được tài liệu hóa dưới đây. Đọc và theo dõi để hoàn thành các phòng lab. Giảng viên của bạn sẽ cung cấp cái nhìn tổng quan về các phòng lab và giúp trả lời bất kỳ câu hỏi nào. Đừng lo lắng nếu bạn gặp khó khăn, chúng tôi cung cấp gợi ý dọc theo quá trình.\nCác mô hình kiến trúc Tiếp theo, bạn sẽ thực hiện Các chuyển đổi dữ liệu Cuối cùng, bạn sẽ áp dụng Machine Learning "
},
{
	"uri": "http://baoandev.github.io/vi/3-ingestion/3.1-skipdms/",
	"title": "Lựa chọn 3: Bỏ qua DMS",
	"tags": [],
	"description": "",
	"content": "Chuẩn bị dữ liệu cho Module 4 và 5 Các bước thực hiện Mở AWS CloudShell Sao chép dữ liệu từ bucket Amazon S3 staging vào bucket S3 của bạn Xác minh dữ liệu Về thiết lập phòng lab Một cơ sở dữ liệu RDS Postgres được sử dụng làm nguồn dữ liệu bán vé cho các sự kiện thể thao. Nó lưu trữ thông tin giao dịch về giá bán vé cho các người được chọn và chuyển nhượng quyền sở hữu vé với các bảng bổ sung cho chi tiết sự kiện. Dịch vụ Di chuyển Cơ sở dữ liệu AWS (DMS) được sử dụng để thực hiện tải dữ liệu đầy đủ từ nguồn Amazon RDS vào bucket Amazon S3.\nTrước khi bắt đầu lab Glue, bạn có thể chọn bỏ qua quá trình di chuyển dữ liệu DMS. Nếu vậy, hãy sao chép dữ liệu nguồn vào bucket S3 của bạn trực tiếp.\nTrong phòng lab hôm nay, bạn sẽ sao chép dữ liệu từ một bucket S3 trung tâm vào tài khoản AWS của bạn, quét tập dữ liệu bằng AWS Glue crawler để tạo metadata, chuyển đổi dữ liệu bằng AWS Glue, truy vấn dữ liệu và tạo View bằng Athena, và cuối cùng xây dựng bảng điều khiển bằng Amazon QuickSight.\nMở AWS CloudShell Mở AWS CloudShell. Nó sẽ mở một cửa sổ terminal trong trình duyệt. (Nếu có một cửa sổ pop-up, hãy đóng nó.)\nSao chép dữ liệu từ bucket Amazon S3 staging vào bucket S3 của bạn Nhập lệnh sau vào terminal, và thay thế tên bucket bằng tên của bạn.\naws s3 cp --recursive --copy-props none s3://aws-dataengineering-day.workshop.aws/data/ s3://\u0026lt;YourBucketName\u0026gt;/tickets/ Xác minh dữ liệu Mở bảng điều khiển S3 và xem dữ liệu đã được sao chép qua terminal CloudShell.\nTên bucket S3 của bạn sẽ trông giống như sau: BucketName/bucket_folder_name/schema_name/table_name/objects/\nTrong ví dụ lab của chúng tôi, điều này trở thành: /\u0026lt;TênBucketCủaBạn\u0026gt;/tickets/dms_sample với một đường dẫn riêng cho từng table_name\n"
},
{
	"uri": "http://baoandev.github.io/vi/7-gluedatabrew/7.2-gluedatabrew/",
	"title": "Chuẩn bị dữ liệu với Glue DataBrew",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://baoandev.github.io/vi/2-realtime/2.2-lab/",
	"title": "Lab: Phát hiện bất thường trong luồng nhấp chuột theo thời gian thực với Dịch vụ Quản lý Amazon cho Apache Flink",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Hướng dẫn này giúp bạn hoàn thành việc phát hiện bất thường trong luồng nhấp chuột theo thời gian thực bằng cách sử dụng Dịch vụ Quản lý Amazon cho Apache Flink.\nPhân tích lưu lượng nhật ký web để có được thông tin chi tiết nhằm đưa ra quyết định kinh doanh thường được thực hiện bằng cách xử lý theo lô. Mặc dù hiệu quả, phương pháp này dẫn đến phản hồi chậm trước các xu hướng mới nổi và hoạt động của người dùng. Có những giải pháp xử lý dữ liệu trong thời gian thực sử dụng công nghệ xử lý luồng và lô nhỏ, nhưng chúng có thể phức tạp để thiết lập và duy trì. Dịch vụ Quản lý Amazon cho Apache Flink là một dịch vụ quản lý giúp dễ dàng nhận dạng và phản ứng với những thay đổi trong hành vi dữ liệu theo thời gian thực.\nCác bước Thiết lập Ứng dụng Amazon Analytics Studio thông qua triển khai stack CloudFormation Tạo lưu lượng truy cập trang web theo thời gian thực bằng Amazon Kinesis Data Generator (KDG) Thực hiện Phân tích Dữ liệu theo thời gian thực Dọn dẹp Môi trường Phụ lục: Kịch bản phát hiện bất thường Trong phần thiết lập trước phòng lab Kinesis, bạn đã hoàn thành các yêu cầu tiên quyết cho phòng lab này. Trong phòng lab này, bạn sẽ tạo một pipeline cho Dịch vụ Quản lý Amazon cho Apache Flink. Thiết lập Ứng dụng Amazon Analytics Studio thông qua triển khai stack CloudFormation Nhấp vào liên kết Deploy to AWS ở đây để thiết lập cơ sở hạ tầng hội thảo trước phòng lab: https://console.aws.amazon.com/cloudformation/home?#/stacks/quickcreate?templateURL=https://aws-bigdata-blog.s3.amazonaws.com/DE-ID-KDA-Lab/1-Lab-Kinesis-Clickstream_CFN.yaml\u0026stackName=kda-flink-pre-lab\u0026param_FlinkVersion=1.15.4\u0026param_Release=master\u0026param_GlueDatabaseName=prelab_kda_db\nLiên kết trên sẽ mở biểu mẫu “Quick create stack”, vui lòng chấp nhận các tham số mặc định, chọn hộp kiểm để xác nhận tạo vai trò IAM mới và chọn Create Stack để chạy Mẫu CloudFormation của Amazon. Stack sẽ tạo sáu Amazon Kinesis Data Streams trong bảng điều khiển Amazon Kinesis:\na. tickerstream – luồng thô để gửi lưu lượng ban đầu\nb. clickstream – ghi lại số lần nhấp chuột\nc. impressionstream – ghi lại số lần hiển thị\nd. ctrstream – ghi lại tỷ lệ nhấp chuột\ne. destinationstream – ghi lại các điểm số bất thường\nf. anomalydetectionstream – ghi lại các bản ghi có điểm số bất thường lớn hơn 2 Mẫu cũng sẽ tạo một ứng dụng Amazon Analytics Studio có tên kda-flink-prelab-RealtimeApplicationNotebook trong Amazon Kinesis Application Console → Tab Studio. Chúng ta sẽ viết Sổ tay Studio tương tác trong Apache Zeppelin để phân tích dữ liệu thời gian thực. Chạy ứng dụng Studio bằng cách chọn kda-flink-prelab-RealtimeApplicationNotebook dưới tab Studio. Chọn “Run” một lần nữa trên màn hình tiếp theo. Tạo lưu lượng truy cập trang web theo thời gian thực bằng Amazon Kinesis Data Generator (KDG) Điều hướng đến bảng điều khiển Amazon CloudFormation trong tài khoản AWS của bạn, nhấp vào stack Kinesis-pre-lab được tạo trong phần thiết lập Prelab Phân tích Dữ liệu Streaming.\nĐi đến tab Outputs của stack để lấy liên kết Kinesis Data Generator như hình dưới.\nMở hai phiên đồng thời của giao diện KDG trong trình duyệt của bạn. Đăng nhập bằng tên người dùng và mật khẩu bạn đã nhập trong mẫu CloudFormation khi tạo phần thiết lập Prelab Phân tích Dữ liệu Streaming.\na. Chúng ta muốn tạo nhiều tin nhắn nhấp chuột hơn tin nhắn hiển thị. Vì vậy, trong phiên đầu tiên, gửi tin nhắn hiển thị với tốc độ một tin nhắn mỗi giây trong 30 giây đến tickerstream, nội dung tin nhắn là:\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;https://www.mysite.com\u0026rdquo;} b. Sau đó, trong phiên thứ hai, gửi tin nhắn nhấp chuột với tốc độ 5 tin nhắn mỗi giây trong 30 giây đến tickerstream, nội dung tin nhắn là:\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;: \u0026ldquo;https://www.mysite.com\u0026rdquo;} c. Bạn có thể xem số lượng tin nhắn được gửi đến luồng dữ liệu khi bạn bắt đầu gửi tin nhắn. d. Sau 30 giây, vui lòng dừng việc gửi cả tin nhắn nhấp chuột và hiển thị.\nThực hiện Phân tích Dữ liệu theo thời gian thực Điều hướng đến bảng điều khiển Amazon Kinesis Application. Dưới tab Studio, chọn kda-prelab-template-RealtimeApplicationNotebook. Chọn “Open in Apache Zeppelin”. Trên bảng điều khiển Apache Zeppelin, chọn Create new note. Đặt tên cho sổ tay là kda_interactive_notebook. Bây giờ hãy thực hiện phân tích tương tác theo thời gian thực với luồng dữ liệu Kinesis. Chúng ta sẽ:\ni. Tạo bảng Flink bằng cách sử dụng các truy vấn SQL của Flink.\nii. Sử dụng truy vấn SQL của Flink để chuyển đổi và tạo luồng dữ liệu mới theo thời gian thực.\niii. Thực hiện phát hiện bất thường bằng Flink User Defined Function và kích hoạt email thông báo bất thường theo thời gian thực.\niv. Các kịch bản có sẵn tại đây.\nhttps://docs.google.com/document/d/1PUBiZUpPkydsek_Navj8f3mskMqE8e_6mjp_rglkq28/edit?usp=sharing\nv. Một sổ tay cũng có sẵn tại đây có thể được tải xuống và nhập qua bảng điều khiển Apache Zeppelin. vi. Bạn có thể mở sổ tay và chạy các đoạn mã theo thứ tự.\nvii. Vui lòng tham khảo logic phát hiện bất thường trong hình dưới đây trước khi bắt đầu thực hiện các bước còn lại. Chạy các đoạn mã tạo bảng. Hàm Người dùng Định nghĩa (UDF) thực hiện phát hiện bất thường theo thời gian thực bằng thuật toán Random Cut Forest. Chạy bước #2. Bạn có thể xem dữ liệu thời gian thực từ các lượt truy cập trang web bằng cách chạy truy vấn ở Bước #3. Tạo impressionstream bằng cách lọc tin nhắn từ tickerstream. Tạo clickstream bằng cách lọc tin nhắn từ tickerstream. Tính toán Tỷ lệ nhấp chuột (CTR) và điền vào ctrstream. Bạn có thể xem Tỷ lệ nhấp chuột theo thời gian thực bằng cách thực hiện Bước #7. Sử dụng UDF (Random Cut Forest) để tạo điểm số bất thường. Điền vào anomalydetectionstream bằng cách thực hiện Bước #9. Bây giờ kiểm tra các điểm số bất thường từ thuật toán Random Cut Forest theo thời gian thực. Bạn sẽ bắt đầu nhận được thông báo trong email của mình khi phát hiện bất thường: Nếu bạn không nhận được email thông báo bất thường lần đầu tiên:\ni. Mở hai phiên đồng thời của giao diện KDG trong trình duyệt của bạn một lần nữa.\nii. Trong phiên đầu tiên, gửi tin nhắn hiển thị với tốc độ một tin nhắn mỗi giây đến tickerstream, nội dung tin nhắn là:\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Impression\u0026rdquo;, \u0026ldquo;site\u0026rdquo;:\u0026ldquo;https://www.mysite.com\u0026rdquo;}\niii. Trong phiên thứ hai, gửi tin nhắn nhấp chuột với tốc độ năm tin nhắn mỗi giây đến tickerstream, nội dung tin nhắn là:\n{\u0026ldquo;browseraction\u0026rdquo;:\u0026ldquo;Click\u0026rdquo;, \u0026ldquo;site\u0026rdquo;:\u0026ldquo;https://www.mysite.com\u0026rdquo;}\niv. Dừng gửi tin nhắn sau 30-40 giây.\nv. Bây giờ trên Sổ tay Apache Zeppelin, lặp lại các bước 3 đến 10 và bạn sẽ bắt đầu nhận được thông báo email từ lần thử thứ hai.\nDọn dẹp Môi trường Sau khi hoàn thành phòng lab, nhấp vào Actions → Stop Application để dừng ứng dụng của bạn và tránh việc nhận quá nhiều tin nhắn SMS và email.\nNếu bạn muốn xóa toàn bộ stack tài nguyên, điều hướng đến Amazon CloudFormation Console → Stacks, chọn kda-flink-pre-lab và nhấp vào Delete để xóa stack. Bước này sẽ dọn dẹp tất cả các tài nguyên đã tạo trước đó. Chọn Delete trên màn hình tiếp theo. "
},
{
	"uri": "http://baoandev.github.io/vi/2-realtime/",
	"title": "Phát hiện bất thường trong luồng nhấp chuột theo thời gian thực với Dịch vụ Quản lý Amazon cho Apache Flink",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Dữ liệu streaming là dữ liệu được tạo ra liên tục từ hàng nghìn nguồn dữ liệu, thường gửi các bản ghi dữ liệu cùng lúc và với kích thước nhỏ (khoảng Kilobytes). Dữ liệu streaming bao gồm nhiều loại dữ liệu khác nhau như tệp log được tạo ra bởi khách hàng khi sử dụng ứng dụng di động hoặc web của bạn, giao dịch mua hàng điện tử, hoạt động của người chơi trong trò chơi, thông tin từ mạng xã hội, sàn giao dịch tài chính, dịch vụ địa lý, hoặc dữ liệu từ thiết bị kết nối hoặc thiết bị đo lường trong các trung tâm dữ liệu.\nDữ liệu này cần được xử lý theo thứ tự và gia tăng dựa trên từng bản ghi hoặc qua các cửa sổ thời gian trượt, và được sử dụng cho nhiều loại phân tích khác nhau như tương quan, tổng hợp, lọc và lấy mẫu. Thông tin từ các phân tích này cung cấp cho các công ty cái nhìn về nhiều khía cạnh của hoạt động kinh doanh và khách hàng của họ như – sử dụng dịch vụ (cho việc đo lường / lập hóa đơn), hoạt động của máy chủ, nhấp chuột trên website, và định vị địa lý của thiết bị, con người và hàng hóa – và cho phép họ phản hồi kịp thời với các tình huống mới phát sinh. Ví dụ, các doanh nghiệp có thể theo dõi sự thay đổi trong cảm xúc công chúng về các thương hiệu và sản phẩm của họ bằng cách phân tích liên tục các dòng dữ liệu xã hội và phản hồi kịp thời khi cần thiết.\nAWS cung cấp một tập hợp công nghệ rất mạnh mẽ như Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Service for Apache Flink và Managed Streaming for Kafka khi làm việc với dữ liệu streaming. Trong lab này, chúng ta sẽ tìm hiểu một số dịch vụ chính với các bài tập thực hành.\nNội dung Thiết lập trước lab cho phát hiện bất thường trong clickstream thời gian thực với Amazon Managed Service for Apache Flink Phát hiện bất thường trong clickstream thời gian thực với Amazon Managed Service for Apache Flink Streaming ETL với Glue "
},
{
	"uri": "http://baoandev.github.io/vi/2-realtime/2.3-labglue/",
	"title": "Lab: ETL theo luồng với Glue",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong lab này, bạn sẽ học cách thu thập, xử lý và tiêu thụ dữ liệu streaming sử dụng các dịch vụ serverless của AWS như Kinesis Data Streams, Glue, S3 và Athena. Để mô phỏng dữ liệu streaming đầu vào, chúng ta sẽ sử dụng Kinesis Data Generator (KDG). Chúng tôi đã giải thích về KDG trong trang chuẩn bị Prelab cho phân tích dữ liệu streaming.\nLab Streaming ETL Thiết lập môi trường Tạo CloudFormation với liên kết bên dưới:\nhttps://console.aws.amazon.com/cloudformation/home#/stacks/create/review?stackName=dmslab-student\u0026amp;templateURL=https://s3.amazonaws.com/aws-dataengineering-day.workshop.aws/DMSlab_student_CFN.yaml\nThiết lập Kinesis Stream Điều hướng đến bảng điều khiển AWS Kinesis. Nhấp vào “Create data stream”. Nhập các thông tin sau: Tên dòng dữ liệu: TicketTransactionStreamingData Chế độ dung lượng: Provisioned Shards đã cấp phát: 2 Nhấp vào “Create data stream”. Hướng dẫn Lab Tạo bảng cho nguồn Kinesis Stream trong Glue Data Catalog Điều hướng đến bảng điều khiển AWS Glue. Trên menu AWS Glue, chọn “Data Catalog” và sau đó là “Tables”, sau đó nhấp vào nút “Add table”. Nhập “TicketTransactionStreamData” làm tên bảng. Nhấp vào “Create database” và nhập “tickettransactiondatabase” làm tên cơ sở dữ liệu, rồi nhấp vào “Create”. Sử dụng menu thả xuống để chọn cơ sở dữ liệu vừa tạo và nhấp vào “Next”. Chọn Kinesis làm nguồn, chọn “Stream in my account” để chọn một dòng dữ liệu Kinesis, chọn khu vực AWS phù hợp nơi bạn đã tạo dòng dữ liệu, chọn tên dòng là “TicketTransactionStreamingData” từ menu thả xuống, chọn JSON làm định dạng dữ liệu đầu vào (vì chúng tôi sẽ gửi các payload JSON từ Kinesis Data Generator trong các bước sau), và nhấp vào “Next”. Để schema trống, vì chúng tôi sẽ bật tính năng phát hiện schema khi định nghĩa công việc Glue stream. Để các chỉ số phân vùng trống. Nhấp vào “Next”. Xem xét tất cả các chi tiết và nhấp vào “Create”. Tạo và kích hoạt công việc Glue Streaming Ở phía bên trái của bảng điều khiển Glue, nhấp vào “Data Integration and ETL” và nhấp vào “ETL jobs”. Nhấp vào nút “Create job from a blank graph”. Đối với nguồn, nhấp vào “Amazon Kinesis” từ danh sách. Nhấp vào nút “Amazon Kinesis” trên canvas, sau đó trong bảng điều khiển bên phải dưới “Data source properties - Kinesis Stream”, cấu hình như sau: Tên: Amazon Kinesis Nguồn Amazon Kinesis: Bảng Data Catalog Cơ sở dữ liệu: tickettransactiondatabase Bảng: tickettransactionstreamdata Đảm bảo rằng “Detect schema” được chọn. Để tất cả các trường khác mặc định. Nhấp vào nút “+” để thêm các nút. Sau đó nhấp vào tab “Targets” và chọn “Amazon S3”. Nhấp vào “Data target - S3 bucket” trên canvas, sau đó trong bảng điều khiển bên phải dưới “Data target properties - S3”, cấu hình như sau: Định dạng: Parquet Loại nén: Không nén Vị trí đích S3: Chọn “Browse S3” và chọn bucket “xxx-xxx-dmslabs3bucket-xxx”. Cuối cùng, chọn tab “Job details” ở trên cùng và cấu hình như sau: Tên: TicketTransactionStreamingJob Vai trò IAM: Chọn “xxx-GlueLabRole-xxx” từ danh sách thả xuống. Loại: Spark Streaming Nhấp vào nút “Save” ở góc trên bên phải để tạo công việc. Khi bạn thấy thông báo “Successfully created job” trên thanh thông báo, nhấp vào nút “Run” để bắt đầu công việc. Kích hoạt dữ liệu streaming từ KDG Khởi chạy KDG sử dụng URL bạn đã đánh dấu từ thiết lập lab. Đăng nhập bằng người dùng và mật khẩu bạn đã cung cấp khi triển khai CloudFormation stack.\nĐảm bảo bạn chọn khu vực phù hợp, từ danh sách thả xuống, chọn “TicketTransactionStreamingData” làm dòng Kinesis mục tiêu, để “Records per second” như mặc định (100 bản ghi mỗi giây); đối với mẫu bản ghi, nhập “NormalTransaction” làm tên payload, và sao chép mẫu payload như sau:\n{ \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Nhấp vào “Send data” để kích hoạt dữ liệu streaming giao dịch mô phỏng.\n"
},
{
	"uri": "http://baoandev.github.io/vi/3-ingestion/",
	"title": "Nhập dữ liệu với DMS",
	"tags": [],
	"description": "",
	"content": "Giới thiệu AWS Database Migration Service (AWS DMS) giúp bạn di chuyển cơ sở dữ liệu đến AWS nhanh chóng và an toàn. Cơ sở dữ liệu nguồn vẫn hoạt động đầy đủ trong quá trình di chuyển, giảm thiểu thời gian ngừng hoạt động cho các ứng dụng phụ thuộc vào cơ sở dữ liệu. AWS Database Migration Service có thể di chuyển dữ liệu của bạn đến và từ các cơ sở dữ liệu thương mại và mã nguồn mở phổ biến nhất.\nAWS Database Migration Service hỗ trợ các di chuyển đồng nhất như Oracle đến Oracle, cũng như di chuyển không đồng nhất giữa các nền tảng cơ sở dữ liệu khác nhau, chẳng hạn như Oracle hoặc Microsoft SQL Server đến Amazon Aurora. Với AWS Database Migration Service, bạn cũng có thể liên tục sao chép dữ liệu với độ trễ thấp từ bất kỳ nguồn nào đến bất kỳ đích nào được hỗ trợ. Ví dụ, bạn có thể sao chép từ nhiều nguồn đến Amazon Simple Storage Service (Amazon S3) để xây dựng một giải pháp hồ dữ liệu có sẵn cao và mở rộng quy mô. Bạn cũng có thể hợp nhất cơ sở dữ liệu thành một kho dữ liệu quy mô petabyte bằng cách phát trực tuyến dữ liệu đến Amazon Redshift. Tìm hiểu thêm về các cơ sở dữ liệu nguồn và đích được hỗ trợ.\nCác nhiệm vụ trong phòng lab sẽ bao gồm bài tập điều hướng sau Phòng lab DMS có ba tùy chọn:\nNếu bạn muốn có trải nghiệm thực hành sâu về DMS (Dịch vụ Di chuyển Dữ liệu) thì trước tiên hãy chạy lab của giảng viên để mô phỏng môi trường cơ sở dữ liệu quan hệ on-prem, sau đó là lab của sinh viên để tạo cơ sở hạ tầng di chuyển dữ liệu cần thiết trong AWS. Lab chính sẽ giúp bạn thực hiện di chuyển dữ liệu thực từ cơ sở dữ liệu quan hệ đến hồ dữ liệu trong AWS. Nếu bạn muốn bắt đầu với Glue ETL và bỏ qua hoàn toàn thực hành DMS, hãy chạy lab tự động hoàn thành DMS. Tự động hoàn thành có thể mất từ 15 đến 20 phút để làm cho tất cả dữ liệu có sẵn trong một lớp hồ dữ liệu S3 trung tâm và bạn sẽ sẵn sàng để thực hiện các chuyển đổi dữ liệu sâu với Glue ETL. Nếu bạn không muốn thực hiện qua phòng lab dịch vụ DMS, bạn có thể sao chép trực tiếp dữ liệu thô vào S3 bằng Tùy chọn 3: Bỏ qua Lab DMS. Hạn chế của tùy chọn này là bạn không thể sử dụng DMS để tạo dữ liệu gia tăng để kiểm tra tính năng bookmark của Glue. "
},
{
	"uri": "http://baoandev.github.io/vi/4-transformingdatawithglue/",
	"title": "Chuyển đổi dữ liệu với Glue",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong lab này, bạn sẽ tìm hiểu về AWS Glue, dịch vụ tích hợp dữ liệu serverless giúp dễ dàng phát hiện, chuẩn bị, di chuyển và tích hợp dữ liệu từ nhiều nguồn khác nhau để phân tích, học máy (ML) và phát triển ứng dụng. Bạn có thể sử dụng crawler để làm đầy AWS Glue Data Catalog với các bảng. Đây là phương pháp chính được hầu hết người dùng AWS Glue sử dụng. Một crawler có thể quét nhiều kho dữ liệu trong một lần chạy. Khi hoàn tất, crawler sẽ tạo hoặc cập nhật một hoặc nhiều bảng trong Data Catalog của bạn. Các công việc Extract, Transform, and Load (ETL) mà bạn định nghĩa trong AWS Glue sử dụng các bảng Data Catalog này làm nguồn và đích. Công việc ETL đọc từ và ghi vào các kho dữ liệu được chỉ định trong các bảng Data Catalog nguồn và đích.\n"
},
{
	"uri": "http://baoandev.github.io/vi/5-queryandvisualize/",
	"title": "Truy vấn và Trực quan hóa",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Phòng lab này giới thiệu cho bạn AWS Glue, Amazon Athena và Amazon QuickSight. AWS Glue là dịch vụ quản lý dữ liệu và ETL hoàn toàn; Amazon Athena cung cấp khả năng thực hiện các truy vấn ad-hoc trên dữ liệu của bạn trong hồ dữ liệu của bạn; và Amazon QuickSight cung cấp trực quan hóa dữ liệu bạn nhập.\n"
},
{
	"uri": "http://baoandev.github.io/vi/7-gluedatabrew/",
	"title": "Glue DataBrew",
	"tags": [],
	"description": "",
	"content": " DataBrew Prelab Data preparation with Glue DataBrew "
},
{
	"uri": "http://baoandev.github.io/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://baoandev.github.io/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]